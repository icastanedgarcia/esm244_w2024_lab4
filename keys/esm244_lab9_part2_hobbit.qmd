---
title: "ESM 244 Lab Week 4 - The Hobbit text analysis"
author: "Casey O'Hara, Nathan Grimes, Allison Horst"
date: "2/1/2024"
format: 
  html:
    code-fold: show
    toc: true
    embed-resources: true
editor: visual
theme: simplex
execute:
  echo: true
  message: false
  warning: false
---

```{r setup}
library(tidyverse)
library(tidytext)
library(pdftools)
library(ggwordcloud)
```

## Get text of The Hobbit

```{r}
hobbit_text <- pdftools::pdf_text(here::here('data', 'the-hobbit.pdf'))
```

- Each row is a page of the PDF (i.e., this is a vector of strings, one for each page)
- Only sees text that is "selectable"

Example: Just want to get text from a single page (e.g. Page 34)? 

```{r}
hobbit_p34 <- hobbit_text[34]
```

## The analysis:

Using the text of The Hobbit by J.R.R. Tolkien:

1. Find the top 5 words per chapter (excluding common and non-informative words).
2. Create a word cloud (just for chapter 1).
3. How does the mood of the text change over the various chapters?

### Pseudocode:




## Prep the data

From Jessica Couture and Casey O'Hara's [text mining workshop](https://github.com/oharac/text_workshop) for eco-data-science: “pdf_text() returns a vector of strings, one for each page of the pdf. So we can mess with it in tidyverse style, let’s turn it into a dataframe, and keep track of the pages. Then we can use stringr::str_split() to break the pages up into individual lines. Each line of the pdf is concluded with a backslash-n, so split on this. We will also add a line number in addition to the page number."

Let's first get it into a data frame. Then we'll do some wrangling with the tidyverse, break it up by chapter, and do some analyses. 

```{r}
hobbit_lines <- data.frame(hobbit_text) %>% 
  mutate(page = 1:n()) %>%
  mutate(text_full = str_split(hobbit_text, pattern = '\\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) 

# Why '\\n' instead of '\n'? Because some symbols (e.g. \, *) need to be called literally with a starting \ to escape the regular expression. For example, \\a for a string actually contains literally \a. So the string that represents the regular expression '\n' is actually '\\n'.

# More information: https://cran.r-project.org/web/packages/stringr/vignettes/regular-expressions.html
```

## Do some tidying

Now, we'll add a new column that contains the Chapter number (so we can use this as a grouping variable later on).

We will use `str_detect()` to look for any cells in "text_full" column that contains the string "Chapter", and if it does, the new column will contain that chapter number:

```{r}
hobbit_chapts <- hobbit_lines %>% 
  slice(-(1:137)) %>% 
  mutate(chapter = ifelse(str_detect(text_full, "Chapter"), text_full, NA)) %>% 
  fill(chapter, .direction = 'down') %>% 
  separate(col = chapter, into = c("ch", "num"), sep = " ") %>% 
  mutate(chapter = as.numeric(as.roman(num)))
```

## Get some word counts by Chapter!

```{r}
hobbit_words <- hobbit_chapts %>% 
  unnest_tokens(word, text_full) %>% 
  select(-hobbit_text)
```

```{r}
hobbit_wordcount <- hobbit_words %>% 
  count(chapter, word)
```

...OK, but check out which words show up the most. They're probably not words we're super interested in (like "a", "the", "and"). How can we limit those? 

## Remove stop words

Those very common (and often uninteresting) words are called "stop words." See `?stop_words` and `View(stop_words)`to look at documentation for stop words lexicons (from the `tidytext` package).

We will *remove* stop words using `tidyr::anti_join()`, which will *omit* any words in `stop_words` from `hobbit_tokens`.

```{r}
head(stop_words)

wordcount_clean <- hobbit_wordcount %>% 
  anti_join(stop_words, by = 'word')
```


## Find the top 5 words from each chapter

```{r}
top_5_words <- wordcount_clean %>% 
  group_by(chapter) %>% 
  arrange(-n) %>% 
  slice(1:5) %>%
  ungroup()

# Make some graphs: 
ggplot(data = top_5_words, aes(x = n, y = word)) +
  geom_col(fill = "blue") +
  facet_wrap(~chapter, scales = "free")
```

## Let's make a word cloud for Chapter 1

```{r}
ch1_top100 <- wordcount_clean %>% 
  filter(chapter == 1) %>% 
  arrange(-n) %>% 
  slice(1:100)
```

```{r}
ch1_cloud <- ggplot(data = ch1_top100, aes(label = word)) +
  geom_text_wordcloud(aes(color = n, size = n), shape = "diamond") +
  scale_size_area(max_size = 6) +
  scale_color_gradientn(colors = c("darkgreen","blue","purple")) +
  theme_minimal()

ch1_cloud
```

## How do sentiments change over the course of the book? 

First, check out the ‘sentiments’ lexicon. From Julia Silge and David Robinson (https://www.tidytextmining.com/sentiment.html):

“The three general-purpose lexicons are

  -  AFINN from Finn Årup Nielsen,
  -  bing from Bing Liu and collaborators, and
  -  nrc (National Research Council Canada) from Saif Mohammad and Peter Turney

All three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth. The nrc lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. The bing lexicon categorizes words in a binary fashion into positive and negative categories. The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment. All of this information is tabulated in the sentiments dataset, and tidytext provides a function get_sentiments() to get specific sentiment lexicons without the columns that are not used in that lexicon."

Let's explore the sentiment lexicons. "bing" included, other lexicons ("afinn", "nrc", "loughran") you'll be prompted to to download.

**WARNING:** These collections include the most offensive words you can think of. 

### afinn lexicon

"afinn": Words ranked from -5 (very negative) to +5 (very positive)

```{r}
afinn_lex <- get_sentiments(lexicon = "afinn")
### you may be prompted to download an updated lexicon - say yes!

# Let's look at the pretty positive words:
afinn_pos <- get_sentiments("afinn") %>% 
  filter(value %in% c(3,4,5))

# Check them out:
DT::datatable(afinn_pos)
```

### bing lexicon

For comparison, check out the bing lexicon: 

```{r}
bing_lex <- get_sentiments(lexicon = "bing")
```

Note the bing lexicon is the simplest of all - just positive/negative!


### nrc lexicon

And the nrc lexicon:https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm includes bins for 8 emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, trust) and positive / negative. 

**Citation for NRC lexicon**: Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.

```{r}
nrc_lex <- get_sentiments(lexicon = "nrc")
```

Note some words fall into multiple emotions!

## Sentiment analysis with bing: 

First, bind words in `hobbit_nonstop_words` to `bing` lexicon:

```{r}
hobbit_bing <- hobbit_words %>% 
  inner_join(bing_lex, by = 'word') ### why inner_join?
```

Let's find some counts of positive vs negative:

```{r}
bing_counts <- hobbit_bing %>% 
  group_by(chapter, sentiment) %>%
  summarize(n = n())

# Plot them: 
ggplot(data = bing_counts, aes(x = sentiment, y = n)) +
  geom_col() +
  facet_wrap(~chapter)
```

But, what if the writer just likes to use more positive or more negative words? How can we balance that out?  Get a book-wide (or catalog-wide) average of sentiment and compare each chapter to that average!

Here log ratios are great!  A simple ratio of + to -, plotted as a bar, will show a wildly positive chapter (10 positives for each negative) as 10, but show a wildly negative chapter (10 negs for each pos) as 0.1.  Log scaling means neutral = 0 (ratio = 1), and wildly positive shows the same length as wildly negative but in opposite directions.

```{r}
# find log ratio score overall:
bing_log_ratio_book <- hobbit_bing %>% 
  summarize(n_pos = sum(sentiment == 'positive'),
            n_neg = sum(sentiment == 'negative'),
            log_ratio = log(n_pos / n_neg))

# Find the log ratio score by chapter: 
bing_log_ratio_ch <- hobbit_bing %>% 
  group_by(chapter) %>% 
  summarize(n_pos = sum(sentiment == 'positive'),
            n_neg = sum(sentiment == 'negative'),
            log_ratio = log(n_pos / n_neg)) %>%
  mutate(log_ratio_adjust = log_ratio - bing_log_ratio_book$log_ratio) %>%
  mutate(pos_neg = ifelse(log_ratio_adjust > 0, 'pos', 'neg'))

ggplot(data = bing_log_ratio_ch, 
       aes(x = log_ratio_adjust,
           y = fct_rev(factor(chapter)),
           fill = pos_neg)) +
           # y = fct_rev(as.factor(chapter)))) +
  geom_col() +
  labs(x = 'Adjusted log(positive/negative)',
       y = 'Chapter number') +
  scale_fill_manual(values = c('pos' = 'slateblue', 'neg' = 'darkred')) +
  theme_minimal() +
  theme(legend.position = 'none')
  
```
### Sentiment analysis with afinn (NOT DONE IN LAB) 

First, bind words in `hobbit_nonstop_words` to `afinn` lexicon:

```{r}
hobbit_afinn <- hobbit_words %>% 
  inner_join(afinn_lex, by = 'word') ### why inner_join?
```

Let's find some counts (by sentiment ranking):

```{r}
afinn_counts <- hobbit_afinn %>% 
  group_by(chapter, value) %>%
  summarize(n = n())

# Plot them: 
ggplot(data = afinn_counts, aes(x = value, y = n)) +
  geom_col() +
  facet_wrap(~chapter)

# Find the mean afinn score by chapter: 
afinn_means <- hobbit_afinn %>% 
  group_by(chapter) %>% 
  summarize(mean_afinn = mean(value))

ggplot(data = afinn_means, 
       aes(y = fct_rev(factor(chapter)),
           x = mean_afinn)) +
  geom_col()
  
```



### Now with NRC lexicon (NOT DONE IN LAB)

Recall, this assigns words to sentiment bins. Let's bind our hobbit data to the NRC lexicon: 

```{r}
hobbit_nrc <- hobbit_words %>% 
  inner_join(get_sentiments("nrc"), by = 'word')
```

Let's find the count of words by chapter and sentiment bin: 

```{r}
hobbit_nrc_counts <- hobbit_nrc %>% 
  group_by(chapter, sentiment) %>%
  summarize(n = n())

ggplot(data = hobbit_nrc_counts, aes(x = n, y = sentiment)) +
  geom_col() +
  facet_wrap(~chapter)
### perhaps order or color the sentiments by positive/negative?

ggplot(data = hobbit_nrc_counts, aes(x = n, 
                                     y = factor(chapter) %>%
                                       fct_rev())) +
  geom_col() +
  facet_wrap(~sentiment) +
  labs(y = 'chapter')

```

